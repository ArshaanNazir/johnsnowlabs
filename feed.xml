<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-06-01T16:35:41+00:00</updated><id>/feed.xml</id><title type="html">Spark NLP</title><subtitle>High Performance NLP with Apache Spark
</subtitle><author><name>{&quot;type&quot;=&gt;nil, &quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;googleplus&quot;=&gt;nil, &quot;telegram&quot;=&gt;nil, &quot;medium&quot;=&gt;nil, &quot;zhihu&quot;=&gt;nil, &quot;douban&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;npm&quot;=&gt;nil}</name></author><entry><title type="html">Financial Finetuned FLAN-T5 Text Generation (FIQA dataset)</title><link href="/2023/05/29/fingen_flant5_finetuned_fiqa_en.html" rel="alternate" type="text/html" title="Financial Finetuned FLAN-T5 Text Generation (FIQA dataset)" /><published>2023-05-29T00:00:00+00:00</published><updated>2023-05-29T00:00:00+00:00</updated><id>/2023/05/29/fingen_flant5_finetuned_fiqa_en</id><content type="html" xml:base="/2023/05/29/fingen_flant5_finetuned_fiqa_en.html">## Description

The `fingen_flant5_finetuned_fiqa` model is the Text Generation model that has been fine-tuned on FLAN-T5 using FIQA dataset. FLAN-T5 is a state-of-the-art language model developed by Facebook AI that utilizes the T5 architecture for text-generation tasks.

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/finance/models/fingen_flant5_finetuned_fiqa_en_1.0.0_3.0_1685363340017.zip){:.button.button-orange.button-orange-trans.arr.button-icon.hidden}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/finance/models/fingen_flant5_finetuned_fiqa_en_1.0.0_3.0_1685363340017.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python

document_assembler = nlp.DocumentAssembler()\
    .setInputCol(&quot;text&quot;)\
    .setOutputCol(&quot;document&quot;)

flant5 = finance.TextGenerator.pretrained(&quot;fingen_flant5_finetuned_fiqa&quot;, &quot;en&quot;, &quot;finance/models&quot;)\
    .setInputCols([&quot;document&quot;])\
    .setOutputCol(&quot;generated&quot;)\
    .setMaxNewTokens(256)\
    .setStopAtEos(True)\
    .setDoSample(True)\
    .setTopK(3)

pipeline = nlp.Pipeline(stages=[document_assembler, flant5])
 
data = spark.createDataFrame([
   [1, &quot;How to have a small capital investment in US if I am out of the country?&quot;]]).toDF('id', 'text')

results = pipeline.fit(data).transform(data)

results.select(&quot;generated.result&quot;).show(truncate=False)

```

&lt;/div&gt;

## Results

```bash

+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|result                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|[I would suggest a local broker. They have diversified funds that are diversified and have the same fees as the US market. They also offer diversified portfolios that have the lowest risk.]|
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|fingen_flant5_finetuned_fiqa|
|Compatibility:|Finance NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Language:|en|
|Size:|1.6 GB|

## References

The dataset is available [here](https://huggingface.co/datasets/BeIR/fiqa)</content><author><name>John Snow Labs</name></author><category term="en" /><category term="finance" /><category term="generation" /><category term="licensed" /><category term="flant5" /><category term="fiqa" /><category term="tensorflow" /><summary type="html">Description The fingen_flant5_finetuned_fiqa model is the Text Generation model that has been fine-tuned on FLAN-T5 using FIQA dataset. FLAN-T5 is a state-of-the-art language model developed by Facebook AI that utilizes the T5 architecture for text-generation tasks. Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) flant5 = finance.TextGenerator.pretrained(&quot;fingen_flant5_finetuned_fiqa&quot;, &quot;en&quot;, &quot;finance/models&quot;)\ .setInputCols([&quot;document&quot;])\ .setOutputCol(&quot;generated&quot;)\ .setMaxNewTokens(256)\ .setStopAtEos(True)\ .setDoSample(True)\ .setTopK(3) pipeline = nlp.Pipeline(stages=[document_assembler, flant5]) data = spark.createDataFrame([ [1, &quot;How to have a small capital investment in US if I am out of the country?&quot;]]).toDF('id', 'text') results = pipeline.fit(data).transform(data) results.select(&quot;generated.result&quot;).show(truncate=False) Results +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |result | +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |[I would suggest a local broker. They have diversified funds that are diversified and have the same fees as the US market. They also offer diversified portfolios that have the lowest risk.]| +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Model Information Model Name: fingen_flant5_finetuned_fiqa Compatibility: Finance NLP 1.0.0+ License: Licensed Edition: Official Language: en Size: 1.6 GB References The dataset is available here</summary></entry><entry><title type="html">Finance FLAN-T5 Question Answering</title><link href="/2023/05/29/finqa_flant5_finetuned_en.html" rel="alternate" type="text/html" title="Finance FLAN-T5 Question Answering" /><published>2023-05-29T00:00:00+00:00</published><updated>2023-05-29T00:00:00+00:00</updated><id>/2023/05/29/finqa_flant5_finetuned_en</id><content type="html" xml:base="/2023/05/29/finqa_flant5_finetuned_en.html">## Description

This Question Answering model has been fine-tuned on FLANT5 using finance data. FLAN-T5 is a state-of-the-art language model developed by Google AI that utilizes the T5 architecture for text generation tasks. This model provides a powerful and efficient solution for accurately answering finance questions and delivering insightful information in the finance domain.

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/finance/models/finqa_flant5_finetuned_en_1.0.0_3.0_1685385263205.zip){:.button.button-orange.button-orange-trans.arr.button-icon.hidden}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/finance/models/finqa_flant5_finetuned_en_1.0.0_3.0_1685385263205.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
document_assembler = nlp.MultiDocumentAssembler()\
    .setInputCols(&quot;question&quot;, &quot;context&quot;)\
    .setOutputCols(&quot;document_question&quot;, &quot;document_context&quot;)

fin_qa = finance.QuestionAnswering.pretrained(&quot;finqa_flant5_finetuned&quot;,&quot;en&quot;,&quot;finance/models&quot;)\
    .setInputCols([&quot;document_question&quot;, &quot;document_context&quot;])\
    .setCustomPrompt(&quot;question: {QUESTION} context: {CONTEXT}&quot;)\
    .setMaxNewTokens(50)\
    .setOutputCol(&quot;answer&quot;)

pipeline = nlp.Pipeline(stages=[document_assembler, fin_qa])

context = &quot;In the world of finance, understanding the concept of risk and return is essential for investors. Risk refers to the uncertainty associated with an investment, while return represents the potential gain or loss. These two factors are intrinsically linked, as higher-risk investments typically offer the potential for higher returns, while lower-risk investments tend to yield lower returns.&quot;
question = &quot;What is the relationship between risk and return in the world of finance?&quot;

data = spark.createDataFrame([[question, context]]).toDF(&quot;question&quot;, &quot;context&quot;)
result = pipeline.fit(data).transform(data)
```

&lt;/div&gt;

## Results

```bash
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|result                                                                                                                                                                                                                                                                                                       |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|[Risk refers to the uncertainty associated with an investment, while return represents the potential gain or loss. These two factors are intrinsically linked, as higher-risk investments typically offer the potential for higher returns, while lower-risk investments tend to yield lower returns.      ]|
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|finqa_flant5_finetuned|
|Compatibility:|Finance NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Language:|en|
|Size:|920.9 MB|
|Case sensitive:|true|

## References

In house annotated dataset</content><author><name>John Snow Labs</name></author><category term="en" /><category term="finance" /><category term="qa" /><category term="question_answering" /><category term="licensed" /><category term="tensorflow" /><summary type="html">Description This Question Answering model has been fine-tuned on FLANT5 using finance data. FLAN-T5 is a state-of-the-art language model developed by Google AI that utilizes the T5 architecture for text generation tasks. This model provides a powerful and efficient solution for accurately answering finance questions and delivering insightful information in the finance domain. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.MultiDocumentAssembler()\ .setInputCols(&quot;question&quot;, &quot;context&quot;)\ .setOutputCols(&quot;document_question&quot;, &quot;document_context&quot;) fin_qa = finance.QuestionAnswering.pretrained(&quot;finqa_flant5_finetuned&quot;,&quot;en&quot;,&quot;finance/models&quot;)\ .setInputCols([&quot;document_question&quot;, &quot;document_context&quot;])\ .setCustomPrompt(&quot;question: {QUESTION} context: {CONTEXT}&quot;)\ .setMaxNewTokens(50)\ .setOutputCol(&quot;answer&quot;) pipeline = nlp.Pipeline(stages=[document_assembler, fin_qa]) context = &quot;In the world of finance, understanding the concept of risk and return is essential for investors. Risk refers to the uncertainty associated with an investment, while return represents the potential gain or loss. These two factors are intrinsically linked, as higher-risk investments typically offer the potential for higher returns, while lower-risk investments tend to yield lower returns.&quot; question = &quot;What is the relationship between risk and return in the world of finance?&quot; data = spark.createDataFrame([[question, context]]).toDF(&quot;question&quot;, &quot;context&quot;) result = pipeline.fit(data).transform(data) Results +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |result | +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |[Risk refers to the uncertainty associated with an investment, while return represents the potential gain or loss. These two factors are intrinsically linked, as higher-risk investments typically offer the potential for higher returns, while lower-risk investments tend to yield lower returns. ]| +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Model Information Model Name: finqa_flant5_finetuned Compatibility: Finance NLP 1.0.0+ License: Licensed Edition: Official Language: en Size: 920.9 MB Case sensitive: true References In house annotated dataset</summary></entry><entry><title type="html">Legal FLAN-T5 Question Answering</title><link href="/2023/05/29/legqa_flant5_finetuned_en.html" rel="alternate" type="text/html" title="Legal FLAN-T5 Question Answering" /><published>2023-05-29T00:00:00+00:00</published><updated>2023-05-29T00:00:00+00:00</updated><id>/2023/05/29/legqa_flant5_finetuned_en</id><content type="html" xml:base="/2023/05/29/legqa_flant5_finetuned_en.html">## Description

This Question Answering model has been fine-tuned on FLANT5 using legal data. FLAN-T5 is a state-of-the-art language model developed by Google AI that utilizes the T5 architecture for text generation tasks. This model provides a powerful and efficient solution for accurately answering legal questions and delivering insightful information in the legal domain.

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/legal/models/legqa_flant5_finetuned_en_1.0.0_3.0_1685371188640.zip){:.button.button-orange.button-orange-trans.arr.button-icon.hidden}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/legal/models/legqa_flant5_finetuned_en_1.0.0_3.0_1685371188640.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
document_assembler = nlp.MultiDocumentAssembler()\
    .setInputCols(&quot;question&quot;, &quot;context&quot;)\
    .setOutputCols(&quot;document_question&quot;, &quot;document_context&quot;)

leg_qa = legal.QuestionAnswering.pretrained(&quot;legqa_flant5_finetuned&quot;,&quot;en&quot;,&quot;legal/models&quot;)\
    .setInputCols([&quot;document_question&quot;, &quot;document_context&quot;])\
    .setCustomPrompt(&quot;question: {QUESTION} context: {CONTEXT}&quot;)\
    .setMaxNewTokens(50)\
    .setOutputCol(&quot;answer&quot;)

pipeline = nlp.Pipeline(stages=[document_assembler, leg_qa])

question = 'How often will the incentive rate be reviewed?'
context = '''

The incentive rate shall remain in effect for a period of one year from the effective date. After the one year period, the incentive rate may be adjusted, or new incentive rates may be put in place, as determined by the governing body of Lincoln Parish, Louisiana. 
The incentive rate shall be reviewed annually by the governing body and any changes or adjustments shall be made in accordance with the terms and conditions of this agreement. Furthermore, the incentive rate shall be adjusted to reflect any changes in the cost of production of the oil or the market price of the oil, as determined by the governing body.
If an adjustment is necessary, the governing body shall notify the parties of such adjustment in writing.'''

data = spark.createDataFrame([[question, context]]).toDF(&quot;question&quot;, &quot;context&quot;)

result = pipeline.fit(data).transform(data)
```

&lt;/div&gt;

## Results

```bash
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|result                                                                                                                                                                  |
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|[The incentive rate shall be reviewed annually by the governing body.                                                                                                  ]|
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|legqa_flant5_finetuned|
|Compatibility:|Legal NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Language:|en|
|Size:|920.9 MB|
|Case sensitive:|true|

## References

In house annotated dataset</content><author><name>John Snow Labs</name></author><category term="en" /><category term="legal" /><category term="qa" /><category term="question_answering" /><category term="licensed" /><category term="tensorflow" /><summary type="html">Description This Question Answering model has been fine-tuned on FLANT5 using legal data. FLAN-T5 is a state-of-the-art language model developed by Google AI that utilizes the T5 architecture for text generation tasks. This model provides a powerful and efficient solution for accurately answering legal questions and delivering insightful information in the legal domain. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.MultiDocumentAssembler()\ .setInputCols(&quot;question&quot;, &quot;context&quot;)\ .setOutputCols(&quot;document_question&quot;, &quot;document_context&quot;) leg_qa = legal.QuestionAnswering.pretrained(&quot;legqa_flant5_finetuned&quot;,&quot;en&quot;,&quot;legal/models&quot;)\ .setInputCols([&quot;document_question&quot;, &quot;document_context&quot;])\ .setCustomPrompt(&quot;question: {QUESTION} context: {CONTEXT}&quot;)\ .setMaxNewTokens(50)\ .setOutputCol(&quot;answer&quot;) pipeline = nlp.Pipeline(stages=[document_assembler, leg_qa]) question = 'How often will the incentive rate be reviewed?' context = ''' The incentive rate shall remain in effect for a period of one year from the effective date. After the one year period, the incentive rate may be adjusted, or new incentive rates may be put in place, as determined by the governing body of Lincoln Parish, Louisiana. The incentive rate shall be reviewed annually by the governing body and any changes or adjustments shall be made in accordance with the terms and conditions of this agreement. Furthermore, the incentive rate shall be adjusted to reflect any changes in the cost of production of the oil or the market price of the oil, as determined by the governing body. If an adjustment is necessary, the governing body shall notify the parties of such adjustment in writing.''' data = spark.createDataFrame([[question, context]]).toDF(&quot;question&quot;, &quot;context&quot;) result = pipeline.fit(data).transform(data) Results +------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |result | +------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |[The incentive rate shall be reviewed annually by the governing body. ]| +------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Model Information Model Name: legqa_flant5_finetuned Compatibility: Legal NLP 1.0.0+ License: Licensed Edition: Official Language: en Size: 920.9 MB Case sensitive: true References In house annotated dataset</summary></entry><entry><title type="html">Financial Finetuned FLAN-T5 Text Generation ( Financial Alpaca )</title><link href="/2023/05/25/fingen_flant5_finetuned_alpaca_en.html" rel="alternate" type="text/html" title="Financial Finetuned FLAN-T5 Text Generation ( Financial Alpaca )" /><published>2023-05-25T00:00:00+00:00</published><updated>2023-05-25T00:00:00+00:00</updated><id>/2023/05/25/fingen_flant5_finetuned_alpaca_en</id><content type="html" xml:base="/2023/05/25/fingen_flant5_finetuned_alpaca_en.html">## Description

The `fingen_flant5_finetuned_alpaca` model is the Text Generation model that has been fine-tuned on FLAN-T5 using Financial Alpaca dataset. FLAN-T5 is a state-of-the-art language model developed by Facebook AI that utilizes the T5 architecture for text-generation tasks.

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/finance/models/fingen_flant5_finetuned_alpaca_en_1.0.0_3.0_1685016665729.zip){:.button.button-orange.button-orange-trans.arr.button-icon.hidden}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/finance/models/fingen_flant5_finetuned_alpaca_en_1.0.0_3.0_1685016665729.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python

document_assembler = nlp.DocumentAssembler()\
    .setInputCol(&quot;text&quot;)\
    .setOutputCol(&quot;document&quot;)

flant5 = finance.TextGenerator.pretrained(&quot;fingen_flant5_finetuned_alpaca&quot;, &quot;en&quot;, &quot;finance/models&quot;)\
    .setInputCols([&quot;document&quot;])\
    .setOutputCol(&quot;generated&quot;)\
    .setMaxNewTokens(256)\
    .setStopAtEos(True)\
    .setDoSample(True)\
    .setTopK(3)

pipeline = nlp.Pipeline(stages=[document_assembler, flant5])
 
data = spark.createDataFrame([
   [1, &quot;What is the US Fair Tax?&quot;]]).toDF('id', 'text')

results = pipeline.fit(data).transform(data)

results.select(&quot;generated.result&quot;).show(truncate=False)

```

&lt;/div&gt;

## Results

```bash

+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|result                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|[Fair tax in the US is essentially an income tax. Fair taxes are tax on your income, and are not taxeable in any country. Fair taxes are taxed as income. If you have a net gain or if the loss of income from taxable activities is less then the fair value (the loss) of your gross income (the loss) then you have to file an Income Report. This will give the US government an overview and give you an understanding. If your net income is less that your fair share of your gross income (which you are entitled) you have the right to claim a refund.]|
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|fingen_flant5_finetuned_alpaca|
|Compatibility:|Finance NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Language:|en|
|Size:|1.6 GB|

## References

The dataset is available [here](https://huggingface.co/datasets/gbharti/finance-alpaca/viewer/gbharti--finance-alpaca)</content><author><name>John Snow Labs</name></author><category term="en" /><category term="finance" /><category term="generation" /><category term="licensed" /><category term="flant5" /><category term="alpaca" /><category term="tensorflow" /><summary type="html">Description The fingen_flant5_finetuned_alpaca model is the Text Generation model that has been fine-tuned on FLAN-T5 using Financial Alpaca dataset. FLAN-T5 is a state-of-the-art language model developed by Facebook AI that utilizes the T5 architecture for text-generation tasks. Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) flant5 = finance.TextGenerator.pretrained(&quot;fingen_flant5_finetuned_alpaca&quot;, &quot;en&quot;, &quot;finance/models&quot;)\ .setInputCols([&quot;document&quot;])\ .setOutputCol(&quot;generated&quot;)\ .setMaxNewTokens(256)\ .setStopAtEos(True)\ .setDoSample(True)\ .setTopK(3) pipeline = nlp.Pipeline(stages=[document_assembler, flant5]) data = spark.createDataFrame([ [1, &quot;What is the US Fair Tax?&quot;]]).toDF('id', 'text') results = pipeline.fit(data).transform(data) results.select(&quot;generated.result&quot;).show(truncate=False) Results +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |result | +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |[Fair tax in the US is essentially an income tax. Fair taxes are tax on your income, and are not taxeable in any country. Fair taxes are taxed as income. If you have a net gain or if the loss of income from taxable activities is less then the fair value (the loss) of your gross income (the loss) then you have to file an Income Report. This will give the US government an overview and give you an understanding. If your net income is less that your fair share of your gross income (which you are entitled) you have the right to claim a refund.]| +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Model Information Model Name: fingen_flant5_finetuned_alpaca Compatibility: Finance NLP 1.0.0+ License: Licensed Edition: Official Language: en Size: 1.6 GB References The dataset is available here</summary></entry><entry><title type="html">Financial Twitter Texts Sentiment Analysis (Large)</title><link href="/2023/05/25/finclf_bert_twitter_financial_text_sentiment_lg_en.html" rel="alternate" type="text/html" title="Financial Twitter Texts Sentiment Analysis (Large)" /><published>2023-05-25T00:00:00+00:00</published><updated>2023-05-25T00:00:00+00:00</updated><id>/2023/05/25/finclf_bert_twitter_financial_text_sentiment_lg_en</id><content type="html" xml:base="/2023/05/25/finclf_bert_twitter_financial_text_sentiment_lg_en.html">## Description

This model is designed to perform sentiment analysis on Twitter data, extracting three primary sentiments: `Bullish`, `Bearish`, and `Neutral`.  This model is the large version of `finclf_bert_twitter_financial_news_sentiment` as it is trained on a much larger dataset.

## Predicted Entities

`Bullish`, `Bearish`, `Neutral`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/finance/models/finclf_bert_twitter_financial_text_sentiment_lg_en_1.0.0_3.0_1684995427342.zip){:.button.button-orange.button-orange-trans.arr.button-icon.hidden}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/finance/models/finclf_bert_twitter_financial_text_sentiment_lg_en_1.0.0_3.0_1684995427342.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
document_assembler = nlp.DocumentAssembler() \
    .setInputCol('text') \
    .setOutputCol('document')

tokenizer = nlp.Tokenizer() \
    .setInputCols(['document']) \
    .setOutputCol('token')

sequenceClassifier = finance.BertForSequenceClassification.pretrained(&quot;finclf_bert_twitter_financial_text_sentiment_lg&quot;, &quot;en&quot;, &quot;finance/models&quot;)\
  .setInputCols([&quot;document&quot;,'token'])\
  .setOutputCol(&quot;class&quot;)
  
pipeline = nlp.Pipeline(stages=[
    document_assembler, 
    tokenizer,
    sequenceClassifier  
])

empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)
model = pipeline.fit(empty_data)

data = [[&quot;&quot;&quot;$GM: Deutsche Bank cuts to Hold &quot;&quot;&quot;],[&quot;&quot;&quot;HELSINKI (Thomson Financial)- Kemira GrowHow swung into profit in its first quarter earnings on improved sales , especially in its fertilizer business in Europe , which is normally stronger during the first quarter .&quot;&quot;&quot;],[&quot;&quot;&quot;Vianor sells tires for cars and trucks as well as a range of other car parts and provides maintenance services .&quot;&quot;&quot;],[&quot;&quot;&quot;Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&amp;D and marketing .&quot;&quot;&quot;]]

# couple of simple examples
example = model.transform(spark.createDataFrame(data).toDF(&quot;text&quot;))

example.select(&quot;text&quot;, &quot;class.result&quot;).show(truncate=False)
```

&lt;/div&gt;

## Results

```bash
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+
|text                                                                                                                                                                                                                    |result   |
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+
|$GM: Deutsche Bank cuts to Hold                                                                                                                                                                                         |[Bearish]|
|HELSINKI (Thomson Financial)- Kemira GrowHow swung into profit in its first quarter earnings on improved sales , especially in its fertilizer business in Europe , which is normally stronger during the first quarter .|[Bullish]|
|Vianor sells tires for cars and trucks as well as a range of other car parts and provides maintenance services .                                                                                                        |[Neutral]|
|Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&amp;D and marketing .                                                                              |[Bearish]|
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|finclf_bert_twitter_financial_text_sentiment_lg|
|Compatibility:|Finance NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[document, token]|
|Output Labels:|[class]|
|Language:|en|
|Size:|406.4 MB|
|Case sensitive:|true|
|Max sentence length:|512|

## References

In-house annotations on financial reports

## Benchmarking

```bash
label             precision    recall  f1-score   support
      Bearish       0.84      0.85      0.84       624
      Bullish       0.90      0.88      0.89      1064
      Neutral       0.94      0.94      0.94      2679
    accuracy          -         -       0.91      4367
    macro-avg       0.89      0.89      0.89      4367
weighted-avg        0.91      0.91      0.91      4367
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="finance" /><category term="bert" /><category term="sentiment" /><category term="licensed" /><category term="tensorflow" /><summary type="html">Description This model is designed to perform sentiment analysis on Twitter data, extracting three primary sentiments: Bullish, Bearish, and Neutral. This model is the large version of finclf_bert_twitter_financial_news_sentiment as it is trained on a much larger dataset. Predicted Entities Bullish, Bearish, Neutral Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = nlp.Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') sequenceClassifier = finance.BertForSequenceClassification.pretrained(&quot;finclf_bert_twitter_financial_text_sentiment_lg&quot;, &quot;en&quot;, &quot;finance/models&quot;)\ .setInputCols([&quot;document&quot;,'token'])\ .setOutputCol(&quot;class&quot;) pipeline = nlp.Pipeline(stages=[ document_assembler, tokenizer, sequenceClassifier ]) empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) model = pipeline.fit(empty_data) data = [[&quot;&quot;&quot;$GM: Deutsche Bank cuts to Hold &quot;&quot;&quot;],[&quot;&quot;&quot;HELSINKI (Thomson Financial)- Kemira GrowHow swung into profit in its first quarter earnings on improved sales , especially in its fertilizer business in Europe , which is normally stronger during the first quarter .&quot;&quot;&quot;],[&quot;&quot;&quot;Vianor sells tires for cars and trucks as well as a range of other car parts and provides maintenance services .&quot;&quot;&quot;],[&quot;&quot;&quot;Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&amp;amp;D and marketing .&quot;&quot;&quot;]] # couple of simple examples example = model.transform(spark.createDataFrame(data).toDF(&quot;text&quot;)) example.select(&quot;text&quot;, &quot;class.result&quot;).show(truncate=False) Results +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+ |text |result | +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+ |$GM: Deutsche Bank cuts to Hold |[Bearish]| |HELSINKI (Thomson Financial)- Kemira GrowHow swung into profit in its first quarter earnings on improved sales , especially in its fertilizer business in Europe , which is normally stronger during the first quarter .|[Bullish]| |Vianor sells tires for cars and trucks as well as a range of other car parts and provides maintenance services . |[Neutral]| |Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&amp;amp;D and marketing . |[Bearish]| +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+ Model Information Model Name: finclf_bert_twitter_financial_text_sentiment_lg Compatibility: Finance NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [document, token] Output Labels: [class] Language: en Size: 406.4 MB Case sensitive: true Max sentence length: 512 References In-house annotations on financial reports Benchmarking label precision recall f1-score support Bearish 0.84 0.85 0.84 624 Bullish 0.90 0.88 0.89 1064 Neutral 0.94 0.94 0.94 2679 accuracy - - 0.91 4367 macro-avg 0.89 0.89 0.89 4367 weighted-avg 0.91 0.91 0.91 4367</summary></entry><entry><title type="html">Financial Twitter News Sentiment Analysis</title><link href="/2023/05/24/finclf_bert_twitter_financial_news_sentiment_en.html" rel="alternate" type="text/html" title="Financial Twitter News Sentiment Analysis" /><published>2023-05-24T00:00:00+00:00</published><updated>2023-05-24T00:00:00+00:00</updated><id>/2023/05/24/finclf_bert_twitter_financial_news_sentiment_en</id><content type="html" xml:base="/2023/05/24/finclf_bert_twitter_financial_news_sentiment_en.html">## Description

This model is designed to perform sentiment analysis on Twitter data, extracting three primary sentiments: `Bullish`, `Bearish`, and `Neutral`.

## Predicted Entities

`Bearish`, `Bullish`, `Neutral`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/finance/models/finclf_bert_twitter_financial_news_sentiment_en_1.0.0_3.0_1684923548358.zip){:.button.button-orange.button-orange-trans.arr.button-icon.hidden}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/finance/models/finclf_bert_twitter_financial_news_sentiment_en_1.0.0_3.0_1684923548358.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
document_assembler = nlp.DocumentAssembler() \
    .setInputCol('text') \
    .setOutputCol('document')

tokenizer = nlp.Tokenizer() \
    .setInputCols(['document']) \
    .setOutputCol('token')

sequenceClassifier = finance.BertForSequenceClassification.pretrained(&quot;finclf_bert_twitter_financial_news_sentiment&quot;, &quot;en&quot;, &quot;finance/models&quot;)\
  .setInputCols([&quot;document&quot;,'token'])\
  .setOutputCol(&quot;class&quot;)
  
pipeline = nlp.Pipeline(stages=[
    document_assembler, 
    tokenizer,
    sequenceClassifier  
])

data = [[&quot;&quot;&quot;$MPLX $MPC - MPLX cut at Credit Suisse on potential dilution from Marathon strategic review https://t.co/0BFQy4ZU6W&quot;&quot;&quot;],[&quot;&quot;&quot;Biogen stock price target raised to $392 from $320 at Instinet&quot;&quot;&quot;],[&quot;&quot;&quot;Luckin Coffee shares halted in premarket; news pending https://t.co/6Kz4NwnNFN&quot;&quot;&quot;]]

empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)

model = pipeline.fit(empty_data)

example = model.transform(spark.createDataFrame(data).toDF(&quot;text&quot;))

example.select(&quot;text&quot;, &quot;class.result&quot;).show(truncate=False)
```

&lt;/div&gt;

## Results

```bash
+-------------------------------------------------------------------------------------------------------------------+---------+
|text                                                                                                               |result   |
+-------------------------------------------------------------------------------------------------------------------+---------+
|$MPLX $MPC - MPLX cut at Credit Suisse on potential dilution from Marathon strategic review https://t.co/0BFQy4ZU6W|[Bearish]|
|Biogen stock price target raised to $392 from $320 at Instinet                                                     |[Bullish]|
|Luckin Coffee shares halted in premarket; news pending https://t.co/6Kz4NwnNFN                                     |[Neutral]|
+-------------------------------------------------------------------------------------------------------------------+---------+
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|finclf_bert_twitter_financial_news_sentiment|
|Compatibility:|Finance NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[document, token]|
|Output Labels:|[class]|
|Language:|en|
|Size:|406.4 MB|
|Case sensitive:|true|
|Max sentence length:|512|

## References

In-house annotations on financial reports

## Benchmarking

```bash
label              precision    recall  f1-score   support
     Bearish       0.80      0.72      0.76       379
     Bullish       0.82      0.78      0.80       468
     Neutral       0.90      0.94      0.92      1540
    accuracy                           0.87      2387
   macro-avg       0.84      0.81      0.83      2387
weighted-avg       0.87      0.87      0.87      2387

```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="finance" /><category term="twitter" /><category term="news" /><category term="sentiment" /><category term="licensed" /><category term="tensorflow" /><summary type="html">Description This model is designed to perform sentiment analysis on Twitter data, extracting three primary sentiments: Bullish, Bearish, and Neutral. Predicted Entities Bearish, Bullish, Neutral Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = nlp.Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') sequenceClassifier = finance.BertForSequenceClassification.pretrained(&quot;finclf_bert_twitter_financial_news_sentiment&quot;, &quot;en&quot;, &quot;finance/models&quot;)\ .setInputCols([&quot;document&quot;,'token'])\ .setOutputCol(&quot;class&quot;) pipeline = nlp.Pipeline(stages=[ document_assembler, tokenizer, sequenceClassifier ]) data = [[&quot;&quot;&quot;$MPLX $MPC - MPLX cut at Credit Suisse on potential dilution from Marathon strategic review https://t.co/0BFQy4ZU6W&quot;&quot;&quot;],[&quot;&quot;&quot;Biogen stock price target raised to $392 from $320 at Instinet&quot;&quot;&quot;],[&quot;&quot;&quot;Luckin Coffee shares halted in premarket; news pending https://t.co/6Kz4NwnNFN&quot;&quot;&quot;]] empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) model = pipeline.fit(empty_data) example = model.transform(spark.createDataFrame(data).toDF(&quot;text&quot;)) example.select(&quot;text&quot;, &quot;class.result&quot;).show(truncate=False) Results +-------------------------------------------------------------------------------------------------------------------+---------+ |text |result | +-------------------------------------------------------------------------------------------------------------------+---------+ |$MPLX $MPC - MPLX cut at Credit Suisse on potential dilution from Marathon strategic review https://t.co/0BFQy4ZU6W|[Bearish]| |Biogen stock price target raised to $392 from $320 at Instinet |[Bullish]| |Luckin Coffee shares halted in premarket; news pending https://t.co/6Kz4NwnNFN |[Neutral]| +-------------------------------------------------------------------------------------------------------------------+---------+ Model Information Model Name: finclf_bert_twitter_financial_news_sentiment Compatibility: Finance NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [document, token] Output Labels: [class] Language: en Size: 406.4 MB Case sensitive: true Max sentence length: 512 References In-house annotations on financial reports Benchmarking label precision recall f1-score support Bearish 0.80 0.72 0.76 379 Bullish 0.82 0.78 0.80 468 Neutral 0.90 0.94 0.92 1540 accuracy 0.87 2387 macro-avg 0.84 0.81 0.83 2387 weighted-avg 0.87 0.87 0.87 2387</summary></entry><entry><title type="html">Financial Twitter Texts Sentiment Analysis</title><link href="/2023/05/24/finclf_bert_twitter_financial_text_sentiment_en.html" rel="alternate" type="text/html" title="Financial Twitter Texts Sentiment Analysis" /><published>2023-05-24T00:00:00+00:00</published><updated>2023-05-24T00:00:00+00:00</updated><id>/2023/05/24/finclf_bert_twitter_financial_text_sentiment_en</id><content type="html" xml:base="/2023/05/24/finclf_bert_twitter_financial_text_sentiment_en.html">## Description

This model is designed to perform sentiment analysis on Twitter data, extracting three primary sentiments: `Positive`, `Negative`, and `Neutral`.

## Predicted Entities

`Positive`, `Negative`, `Neutral`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/finance/models/finclf_bert_twitter_financial_text_sentiment_en_1.0.0_3.0_1684941800385.zip){:.button.button-orange.button-orange-trans.arr.button-icon.hidden}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/finance/models/finclf_bert_twitter_financial_text_sentiment_en_1.0.0_3.0_1684941800385.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
document_assembler = nlp.DocumentAssembler() \
    .setInputCol('text') \
    .setOutputCol('document')

tokenizer = nlp.Tokenizer() \
    .setInputCols(['document']) \
    .setOutputCol('token')

sequenceClassifier = finance.BertForSequenceClassification.pretrained(&quot;finclf_bert_twitter_financial_text_sentiment&quot;, &quot;en&quot;, &quot;finance/models&quot;)\
  .setInputCols([&quot;document&quot;,'token'])\
  .setOutputCol(&quot;class&quot;)
  
pipeline = nlp.Pipeline(stages=[
    document_assembler, 
    tokenizer,
    sequenceClassifier  
])


empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)

model = pipeline.fit(empty_data)

data = [[&quot;&quot;&quot;Early Crater Lake Drill Results Return Better Than Expected Grades and Intersection Lengths â€“ 79.7 meters at 311 g/t Scandium Oxide, 0.326% Rare Earths Oxides and Yttrium --  Imperial Mining Group Ltd. (&quot;Imperial&quot;) (TSX VENTURE: IPG; OTCQB: IMPNF) is pleased to announce that it has completed its Summer 2022 exploration and definition diamond drill program on the Ta-Nb Target and the TG Zone. Early results are encouraging and give inference to grade and tonnage increases to the TG North Lobe Deposit resource (see Imperial Press release - SEP 23, 2021).&quot;&quot;&quot;],[&quot;&quot;&quot;Noranda Income Fund Provides an Update on Operational and Production Challenges and Announces a Cellhouse Maintenance Shutdown --  Noranda Income Fund (TSX:NIF.UN) (the â€œFundâ€) today provided an update regarding its previously disclosed challenges with cellhouse operating conditions and equipment fragility, which have been adversely affecting zinc production volumes and output quality.&quot;&quot;&quot;]]

# couple of simple examples
example = model.transform(spark.createDataFrame(data).toDF(&quot;text&quot;))

example.select(&quot;text&quot;, &quot;class.result&quot;).show(truncate=False)
```

&lt;/div&gt;

## Results

```bash
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+
|text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |result    |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+
|Early Crater Lake Drill Results Return Better Than Expected Grades and Intersection Lengths â€“ 79.7 meters at 311 g/t Scandium Oxide, 0.326% Rare Earths Oxides and Yttrium --  Imperial Mining Group Ltd. (&quot;Imperial&quot;) (TSX VENTURE: IPG; OTCQB: IMPNF) is pleased to announce that it has completed its Summer 2022 exploration and definition diamond drill program on the Ta-Nb Target and the TG Zone. Early results are encouraging and give inference to grade and tonnage increases to the TG North Lobe Deposit resource (see Imperial Press release - SEP 23, 2021).|[Positive]|
|Noranda Income Fund Provides an Update on Operational and Production Challenges and Announces a Cellhouse Maintenance Shutdown --  Noranda Income Fund (TSX:NIF.UN) (the â€œFundâ€) today provided an update regarding its previously disclosed challenges with cellhouse operating conditions and equipment fragility, which have been adversely affecting zinc production volumes and output quality.                                                                                                                                                                       |[Negative]|
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+


```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|finclf_bert_twitter_financial_text_sentiment|
|Compatibility:|Finance NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[document, token]|
|Output Labels:|[class]|
|Language:|en|
|Size:|406.4 MB|
|Case sensitive:|true|
|Max sentence length:|512|

## References

In-house annotations on financial reports

## Benchmarking

```bash
label              precision    recall  f1-score   support
    Negative       0.75      0.60      0.67        15
     Neutral       0.89      0.87      0.88       207
    Positive       0.83      0.87      0.85       134
    accuracy         -          -      0.86       356
   macro-avg       0.82      0.78      0.80       356
weighted-avg       0.86      0.86      0.86       356

```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="finance" /><category term="sentiment" /><category term="bert" /><category term="licensed" /><category term="tensorflow" /><summary type="html">Description This model is designed to perform sentiment analysis on Twitter data, extracting three primary sentiments: Positive, Negative, and Neutral. Predicted Entities Positive, Negative, Neutral Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = nlp.Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') sequenceClassifier = finance.BertForSequenceClassification.pretrained(&quot;finclf_bert_twitter_financial_text_sentiment&quot;, &quot;en&quot;, &quot;finance/models&quot;)\ .setInputCols([&quot;document&quot;,'token'])\ .setOutputCol(&quot;class&quot;) pipeline = nlp.Pipeline(stages=[ document_assembler, tokenizer, sequenceClassifier ]) empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) model = pipeline.fit(empty_data) data = [[&quot;&quot;&quot;Early Crater Lake Drill Results Return Better Than Expected Grades and Intersection Lengths â€“ 79.7 meters at 311 g/t Scandium Oxide, 0.326% Rare Earths Oxides and Yttrium -- Imperial Mining Group Ltd. (&quot;Imperial&quot;) (TSX VENTURE: IPG; OTCQB: IMPNF) is pleased to announce that it has completed its Summer 2022 exploration and definition diamond drill program on the Ta-Nb Target and the TG Zone. Early results are encouraging and give inference to grade and tonnage increases to the TG North Lobe Deposit resource (see Imperial Press release - SEP 23, 2021).&quot;&quot;&quot;],[&quot;&quot;&quot;Noranda Income Fund Provides an Update on Operational and Production Challenges and Announces a Cellhouse Maintenance Shutdown -- Noranda Income Fund (TSX:NIF.UN) (the â€œFundâ€) today provided an update regarding its previously disclosed challenges with cellhouse operating conditions and equipment fragility, which have been adversely affecting zinc production volumes and output quality.&quot;&quot;&quot;]] # couple of simple examples example = model.transform(spark.createDataFrame(data).toDF(&quot;text&quot;)) example.select(&quot;text&quot;, &quot;class.result&quot;).show(truncate=False) Results +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+ |text |result | +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+ |Early Crater Lake Drill Results Return Better Than Expected Grades and Intersection Lengths â€“ 79.7 meters at 311 g/t Scandium Oxide, 0.326% Rare Earths Oxides and Yttrium -- Imperial Mining Group Ltd. (&quot;Imperial&quot;) (TSX VENTURE: IPG; OTCQB: IMPNF) is pleased to announce that it has completed its Summer 2022 exploration and definition diamond drill program on the Ta-Nb Target and the TG Zone. Early results are encouraging and give inference to grade and tonnage increases to the TG North Lobe Deposit resource (see Imperial Press release - SEP 23, 2021).|[Positive]| |Noranda Income Fund Provides an Update on Operational and Production Challenges and Announces a Cellhouse Maintenance Shutdown -- Noranda Income Fund (TSX:NIF.UN) (the â€œFundâ€) today provided an update regarding its previously disclosed challenges with cellhouse operating conditions and equipment fragility, which have been adversely affecting zinc production volumes and output quality. |[Negative]| +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+ Model Information Model Name: finclf_bert_twitter_financial_text_sentiment Compatibility: Finance NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [document, token] Output Labels: [class] Language: en Size: 406.4 MB Case sensitive: true Max sentence length: 512 References In-house annotations on financial reports Benchmarking label precision recall f1-score support Negative 0.75 0.60 0.67 15 Neutral 0.89 0.87 0.88 207 Positive 0.83 0.87 0.85 134 accuracy - - 0.86 356 macro-avg 0.82 0.78 0.80 356 weighted-avg 0.86 0.86 0.86 356</summary></entry><entry><title type="html">Medical Question Answering (biogpt)</title><link href="/2023/05/17/medical_qa_biogpt_en.html" rel="alternate" type="text/html" title="Medical Question Answering (biogpt)" /><published>2023-05-17T00:00:00+00:00</published><updated>2023-05-17T00:00:00+00:00</updated><id>/2023/05/17/medical_qa_biogpt_en</id><content type="html" xml:base="/2023/05/17/medical_qa_biogpt_en.html">## Description

This model is directly ported from the  official BioGPT [implementation](https://github.com/microsoft/BioGPT)  that is trained on Pubmed abstracts and then finetuned with PubmedQA dataset. It is the baseline version called [BioGPT-QA-PubMedQA-BioGPT](https://msramllasc.blob.core.windows.net/modelrelease/BioGPT/checkpoints/QA-PubMedQA-BioGPT.tgz).
It can generate two types of answers, short and long. Types of questions are supported: `&quot;short&quot;`(producing yes/no/maybe) answers and `&quot;full&quot;` (long answers).

## Predicted Entities



{:.btn-box}
[Live Demo](https://demo.johnsnowlabs.com/healthcare/BIOGPT_MEDICAL_QUESTION_ANSWERING/){:.button.button-orange}
[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/31.Medical_Question_Answering.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/medical_qa_biogpt_en_4.4.2_3.0_1684313829161.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/clinical/models/medical_qa_biogpt_en_4.4.2_3.0_1684313829161.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
document_assembler = MultiDocumentAssembler()\
    .setInputCols(&quot;question&quot;, &quot;context&quot;)\
    .setOutputCols(&quot;document_question&quot;, &quot;document_context&quot;)

med_qa = sparknlp_jsl.annotators.MedicalQuestionAnswering\
    .pretrained(&quot;medical_qa_biogpt&quot;,&quot;en&quot;,&quot;clinical/models&quot;)\
    .setInputCols([&quot;document_question&quot;, &quot;document_context&quot;])\
    .setOutputCol(&quot;answer&quot;)\
    .setMaxNewTokens(30)\
    .setTopK(1)\
    .setQuestionType(&quot;long&quot;) # &quot;short&quot;

pipeline = Pipeline(stages=[document_assembler, med_qa])

paper_abstract = &quot;The visual indexing theory proposed by Zenon Pylyshyn (Cognition, 32, 65–97, 1989) predicts that visual attention mechanisms are employed when mental images are projected onto a visual scene. Recent eye-tracking studies have supported this hypothesis by showing that people tend to look at empty places where requested information has been previously presented. However, it has remained unclear to what extent this behavior is related to memory performance. The aim of the present study was to explore whether the manipulation of spatial attention can facilitate memory retrieval. In two experiments, participants were asked first to memorize a set of four objects and then to determine whether a probe word referred to any of the objects. The results of both experiments indicate that memory accuracy is not affected by the current focus of attention and that all the effects of directing attention to specific locations on response times can be explained in terms of stimulus–stimulus and stimulus–response spatial compatibility.&quot;

long_question = &quot;What is the effect of directing attention on memory?&quot;
yes_no_question = &quot;Does directing attention improve memory for items?&quot;

data = spark.createDataFrame(
    [
        [long_question, paper_abstract, &quot;long&quot;],
        [yes_no_question, paper_abstract, &quot;short&quot;],
    ]
).toDF(&quot;question&quot;, &quot;context&quot;, &quot;question_type&quot;)

pipeline.fit(data).transform(data.where(&quot;question_type == 'long'&quot;))\
    .select(&quot;answer.result&quot;)\
    .show(truncate=False)


###############################
# for short ansver

med_qa.setQuestionType(&quot;short&quot;) # &quot;long&quot;

pipeline = Pipeline(stages=[document_assembler, med_qa])

pipeline.fit(data).transform(data.where(&quot;question_type == 'short'&quot;))\
    .select(&quot;answer.result&quot;)\
    .show(truncate=False)
```
```scala
val document_assembler = new MultiDocumentAssembler()
    .setInputCols(&quot;question&quot;, &quot;context&quot;)
    .setOutputCols(&quot;document_question&quot;, &quot;document_context&quot;)

val med_qa = MedicalQuestionAnswering
    .pretrained(&quot;medical_qa_biogpt&quot;, &quot;en&quot;, &quot;clinical/models&quot;)
    .setInputCols(Array(&quot;document_question&quot;, &quot;document_context&quot;))
    .setOutputCol(&quot;answer&quot;)
    .setMaxNewTokens(30)
    .setTopK(1)
    .setQuestionType(&quot;long&quot;) # &quot;short&quot;

val pipeline = new Pipeline().setStages(Array(document_assembler, med_qa))

paper_abstract = &quot;The visual indexing theory proposed by Zenon Pylyshyn (Cognition, 32, 65–97, 1989) predicts that visual attention mechanisms are employed when mental images are projected onto a visual scene. Recent eye-tracking studies have supported this hypothesis by showing that people tend to look at empty places where requested information has been previously presented. However, it has remained unclear to what extent this behavior is related to memory performance. The aim of the present study was to explore whether the manipulation of spatial attention can facilitate memory retrieval. In two experiments, participants were asked first to memorize a set of four objects and then to determine whether a probe word referred to any of the objects. The results of both experiments indicate that memory accuracy is not affected by the current focus of attention and that all the effects of directing attention to specific locations on response times can be explained in terms of stimulus–stimulus and stimulus–response spatial compatibility.&quot;

long_question = &quot;What is the effect of directing attention on memory?&quot;
yes_no_question = &quot;Does directing attention improve memory for items?&quot;

val data = Seq( 
    (long_question, paper_abstract,&quot;long&quot; ),
    (yes_no_question, paper_abstract, &quot;short&quot;))
    .toDS.toDF(&quot;question&quot;, &quot;context&quot;, &quot;question_type&quot;)

val result = pipeline.fit(data).transform(data)
```
&lt;/div&gt;

## Results

```bash
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|result                                                                                                                                                                        |
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|[The effect of directing attention on memory is that it can help to improve the accuracy and recall of a document. It can help to improve the accuracy of a document by allowing the user to quickly and easily access the information they need. It can also help to improve the overall efficiency of a document by allowing the user to quickly]|
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|medical_qa_biogpt|
|Compatibility:|Healthcare NLP 4.4.2+|
|License:|Licensed|
|Edition:|Official|
|Language:|en|
|Size:|1.1 GB|
|Case sensitive:|true|</content><author><name>John Snow Labs</name></author><category term="licensed" /><category term="clinical" /><category term="en" /><category term="biogpt" /><category term="pubmed" /><category term="question_answering" /><category term="tensorflow" /><summary type="html">Description This model is directly ported from the  official BioGPT implementation  that is trained on Pubmed abstracts and then finetuned with PubmedQA dataset. It is the baseline version called BioGPT-QA-PubMedQA-BioGPT. It can generate two types of answers, short and long. Types of questions are supported: &quot;short&quot;(producing yes/no/maybe) answers and &quot;full&quot; (long answers). Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = MultiDocumentAssembler()\ .setInputCols(&quot;question&quot;, &quot;context&quot;)\ .setOutputCols(&quot;document_question&quot;, &quot;document_context&quot;) med_qa = sparknlp_jsl.annotators.MedicalQuestionAnswering\ .pretrained(&quot;medical_qa_biogpt&quot;,&quot;en&quot;,&quot;clinical/models&quot;)\ .setInputCols([&quot;document_question&quot;, &quot;document_context&quot;])\ .setOutputCol(&quot;answer&quot;)\ .setMaxNewTokens(30)\ .setTopK(1)\ .setQuestionType(&quot;long&quot;) # &quot;short&quot; pipeline = Pipeline(stages=[document_assembler, med_qa]) paper_abstract = &quot;The visual indexing theory proposed by Zenon Pylyshyn (Cognition, 32, 65–97, 1989) predicts that visual attention mechanisms are employed when mental images are projected onto a visual scene. Recent eye-tracking studies have supported this hypothesis by showing that people tend to look at empty places where requested information has been previously presented. However, it has remained unclear to what extent this behavior is related to memory performance. The aim of the present study was to explore whether the manipulation of spatial attention can facilitate memory retrieval. In two experiments, participants were asked first to memorize a set of four objects and then to determine whether a probe word referred to any of the objects. The results of both experiments indicate that memory accuracy is not affected by the current focus of attention and that all the effects of directing attention to specific locations on response times can be explained in terms of stimulus–stimulus and stimulus–response spatial compatibility.&quot; long_question = &quot;What is the effect of directing attention on memory?&quot; yes_no_question = &quot;Does directing attention improve memory for items?&quot; data = spark.createDataFrame( [ [long_question, paper_abstract, &quot;long&quot;], [yes_no_question, paper_abstract, &quot;short&quot;], ] ).toDF(&quot;question&quot;, &quot;context&quot;, &quot;question_type&quot;) pipeline.fit(data).transform(data.where(&quot;question_type == 'long'&quot;))\ .select(&quot;answer.result&quot;)\ .show(truncate=False) ############################### # for short ansver med_qa.setQuestionType(&quot;short&quot;) # &quot;long&quot; pipeline = Pipeline(stages=[document_assembler, med_qa]) pipeline.fit(data).transform(data.where(&quot;question_type == 'short'&quot;))\ .select(&quot;answer.result&quot;)\ .show(truncate=False) val document_assembler = new MultiDocumentAssembler() .setInputCols(&quot;question&quot;, &quot;context&quot;) .setOutputCols(&quot;document_question&quot;, &quot;document_context&quot;) val med_qa = MedicalQuestionAnswering .pretrained(&quot;medical_qa_biogpt&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;document_question&quot;, &quot;document_context&quot;)) .setOutputCol(&quot;answer&quot;) .setMaxNewTokens(30) .setTopK(1) .setQuestionType(&quot;long&quot;) # &quot;short&quot; val pipeline = new Pipeline().setStages(Array(document_assembler, med_qa)) paper_abstract = &quot;The visual indexing theory proposed by Zenon Pylyshyn (Cognition, 32, 65–97, 1989) predicts that visual attention mechanisms are employed when mental images are projected onto a visual scene. Recent eye-tracking studies have supported this hypothesis by showing that people tend to look at empty places where requested information has been previously presented. However, it has remained unclear to what extent this behavior is related to memory performance. The aim of the present study was to explore whether the manipulation of spatial attention can facilitate memory retrieval. In two experiments, participants were asked first to memorize a set of four objects and then to determine whether a probe word referred to any of the objects. The results of both experiments indicate that memory accuracy is not affected by the current focus of attention and that all the effects of directing attention to specific locations on response times can be explained in terms of stimulus–stimulus and stimulus–response spatial compatibility.&quot; long_question = &quot;What is the effect of directing attention on memory?&quot; yes_no_question = &quot;Does directing attention improve memory for items?&quot; val data = Seq( (long_question, paper_abstract,&quot;long&quot; ), (yes_no_question, paper_abstract, &quot;short&quot;)) .toDS.toDF(&quot;question&quot;, &quot;context&quot;, &quot;question_type&quot;) val result = pipeline.fit(data).transform(data) Results +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |result | +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |[The effect of directing attention on memory is that it can help to improve the accuracy and recall of a document. It can help to improve the accuracy of a document by allowing the user to quickly and easily access the information they need. It can also help to improve the overall efficiency of a document by allowing the user to quickly]| +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Model Information Model Name: medical_qa_biogpt Compatibility: Healthcare NLP 4.4.2+ License: Licensed Edition: Official Language: en Size: 1.1 GB Case sensitive: true</summary></entry><entry><title type="html">Understanding Restriction Level of Assignment Clauses(Bert)</title><link href="/2023/05/17/legclf_nda_assigments_bert_en.html" rel="alternate" type="text/html" title="Understanding Restriction Level of Assignment Clauses(Bert)" /><published>2023-05-17T00:00:00+00:00</published><updated>2023-05-17T00:00:00+00:00</updated><id>/2023/05/17/legclf_nda_assigments_bert_en</id><content type="html" xml:base="/2023/05/17/legclf_nda_assigments_bert_en.html">## Description

Given a clause classified as `ASSIGNMENT ` using the `legmulticlf_mnda_sections_paragraph_other` classifier, you can subclassify the sentences as `PERMISSIVE_ASSIGNMENT`, `RESTRICTIVE_ASSIGNMENT` or `OTHER` from it using the `legclf_nda_assigments_bert` model. It has been trained with the SOTA approach.

## Predicted Entities

`PERMISSIVE_ASSIGNMENT`, `RESTRICTIVE_ASSIGNMENT`, `OTHER`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/legal/models/legclf_nda_assigments_bert_en_1.0.0_3.0_1684350248553.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/legal/models/legclf_nda_assigments_bert_en_1.0.0_3.0_1684350248553.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
document_assembler = nlp.DocumentAssembler()\
    .setInputCol(&quot;text&quot;)\
    .setOutputCol(&quot;document&quot;)

tokenizer = nlp.Tokenizer()\
    .setInputCols([&quot;document&quot;])\
    .setOutputCol(&quot;token&quot;)

sequence_classifier = legal.BertForSequenceClassification.pretrained(&quot;legclf_nda_assigments_bert&quot;, &quot;en&quot;, &quot;legal/models&quot;)\
    .setInputCols([&quot;document&quot;,&quot;token&quot;])\
    .setOutputCol(&quot;class&quot;)\
    .setCaseSensitive(True)\
    .setMaxSentenceLength(512)

clf_pipeline = nlp.Pipeline(stages=[
    document_assembler, 
    tokenizer,
    sequence_classifier    
])

empty_df = spark.createDataFrame([['']]).toDF(&quot;text&quot;)

model = clf_pipeline.fit(empty_df)

text_list = [
&quot;&quot;&quot;This Agreement will be binding upon and inure to the benefit of each Party and its respective heirs, successors and assigns&quot;&quot;&quot;,
&quot;&quot;&quot;All notices and other communications provided for in this Agreement and the other Loan Documents shall be in writing and may (subject to paragraph (b) below) be telecopied (faxed), mailed by certified mail return receipt requested, or delivered by hand or overnight courier service to the intended recipient at the addresses specified below or at such other address as shall be designated by any party listed below in a notice to the other parties listed below given in accordance with this Section.&quot;&quot;&quot;,
&quot;&quot;&quot;This Agreement is a personal contract for XCorp, and the rights and interests of XCorp hereunder may not be sold, transferred, assigned, pledged or hypothecated except as otherwise expressly permitted by the Company&quot;&quot;&quot;
]

df = spark.createDataFrame(pd.DataFrame({&quot;text&quot; : text_list}))

result = model.transform(df)
```

&lt;/div&gt;

## Results

```bash
+--------------------------------------------------------------------------------+----------------------+
|                                                                            text|                 class|
+--------------------------------------------------------------------------------+----------------------+
|This Agreement will be binding upon and inure to the benefit of each Party an...| PERMISSIVE_ASSIGNMENT|
|All notices and other communications provided for in this Agreement and the o...|                 OTHER|
|This Agreement is a personal contract for XCorp, and the rights and interests...|RESTRICTIVE_ASSIGNMENT|
+--------------------------------------------------------------------------------+----------------------+
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|legclf_nda_assigments_bert|
|Compatibility:|Legal NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[document, token]|
|Output Labels:|[class]|
|Language:|en|
|Size:|406.4 MB|
|Case sensitive:|true|
|Max sentence length:|512|

## Sample text from the training dataset

In-house annotations on the Non-disclosure Agreements

## Benchmarking

```bash
label                   precision  recall  f1-score  support 
OTHER                   0.98       1.00    0.99      124     
PERMISSIVE_ASSIGNMENT   1.00       0.93    0.97      15      
RESTRICTIVE_ASSIGNMENT  1.00       0.96    0.98      25      
accuracy                -          -       0.99      164     
macro avg               0.99       0.96    0.98      164     
weighted avg            0.99       0.99    0.99      164     
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="legal" /><category term="licensed" /><category term="bert" /><category term="nda" /><category term="classification" /><category term="assigments" /><category term="tensorflow" /><summary type="html">Description Given a clause classified as ASSIGNMENT using the legmulticlf_mnda_sections_paragraph_other classifier, you can subclassify the sentences as PERMISSIVE_ASSIGNMENT, RESTRICTIVE_ASSIGNMENT or OTHER from it using the legclf_nda_assigments_bert model. It has been trained with the SOTA approach. Predicted Entities PERMISSIVE_ASSIGNMENT, RESTRICTIVE_ASSIGNMENT, OTHER Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) tokenizer = nlp.Tokenizer()\ .setInputCols([&quot;document&quot;])\ .setOutputCol(&quot;token&quot;) sequence_classifier = legal.BertForSequenceClassification.pretrained(&quot;legclf_nda_assigments_bert&quot;, &quot;en&quot;, &quot;legal/models&quot;)\ .setInputCols([&quot;document&quot;,&quot;token&quot;])\ .setOutputCol(&quot;class&quot;)\ .setCaseSensitive(True)\ .setMaxSentenceLength(512) clf_pipeline = nlp.Pipeline(stages=[ document_assembler, tokenizer, sequence_classifier ]) empty_df = spark.createDataFrame([['']]).toDF(&quot;text&quot;) model = clf_pipeline.fit(empty_df) text_list = [ &quot;&quot;&quot;This Agreement will be binding upon and inure to the benefit of each Party and its respective heirs, successors and assigns&quot;&quot;&quot;, &quot;&quot;&quot;All notices and other communications provided for in this Agreement and the other Loan Documents shall be in writing and may (subject to paragraph (b) below) be telecopied (faxed), mailed by certified mail return receipt requested, or delivered by hand or overnight courier service to the intended recipient at the addresses specified below or at such other address as shall be designated by any party listed below in a notice to the other parties listed below given in accordance with this Section.&quot;&quot;&quot;, &quot;&quot;&quot;This Agreement is a personal contract for XCorp, and the rights and interests of XCorp hereunder may not be sold, transferred, assigned, pledged or hypothecated except as otherwise expressly permitted by the Company&quot;&quot;&quot; ] df = spark.createDataFrame(pd.DataFrame({&quot;text&quot; : text_list})) result = model.transform(df) Results +--------------------------------------------------------------------------------+----------------------+ | text| class| +--------------------------------------------------------------------------------+----------------------+ |This Agreement will be binding upon and inure to the benefit of each Party an...| PERMISSIVE_ASSIGNMENT| |All notices and other communications provided for in this Agreement and the o...| OTHER| |This Agreement is a personal contract for XCorp, and the rights and interests...|RESTRICTIVE_ASSIGNMENT| +--------------------------------------------------------------------------------+----------------------+ Model Information Model Name: legclf_nda_assigments_bert Compatibility: Legal NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [document, token] Output Labels: [class] Language: en Size: 406.4 MB Case sensitive: true Max sentence length: 512 Sample text from the training dataset In-house annotations on the Non-disclosure Agreements Benchmarking label precision recall f1-score support OTHER 0.98 1.00 0.99 124 PERMISSIVE_ASSIGNMENT 1.00 0.93 0.97 15 RESTRICTIVE_ASSIGNMENT 1.00 0.96 0.98 25 accuracy - - 0.99 164 macro avg 0.99 0.96 0.98 164 weighted avg 0.99 0.99 0.99 164</summary></entry><entry><title type="html">Understanding Non-compete Items in Non-Compete Clauses (Bert)</title><link href="/2023/05/17/legclf_nda_non_compete_items_bert_en.html" rel="alternate" type="text/html" title="Understanding Non-compete Items in Non-Compete Clauses (Bert)" /><published>2023-05-17T00:00:00+00:00</published><updated>2023-05-17T00:00:00+00:00</updated><id>/2023/05/17/legclf_nda_non_compete_items_bert_en</id><content type="html" xml:base="/2023/05/17/legclf_nda_non_compete_items_bert_en.html">## Description

Given a clause classified as `NON_COMP` using the `legmulticlf_mnda_sections_paragraph_other` classifier, you can subclassify the sentences as `NON_COMPETE_ITEMS`, or `OTHER` from it using the `legclf_nda_non_compete_items_bert` model. It has been trained with the SOTA approach.

## Predicted Entities

`NON_COMPETE_ITEMS`, `OTHER`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/legal/models/legclf_nda_non_compete_items_bert_en_1.0.0_3.0_1684358961459.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/legal/models/legclf_nda_non_compete_items_bert_en_1.0.0_3.0_1684358961459.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
document_assembler = nlp.DocumentAssembler()\
    .setInputCol(&quot;text&quot;)\
    .setOutputCol(&quot;document&quot;)

tokenizer = nlp.Tokenizer()\
    .setInputCols([&quot;document&quot;])\
    .setOutputCol(&quot;token&quot;)

sequence_classifier = legal.BertForSequenceClassification.pretrained(&quot;legclf_nda_non_compete_items_bert&quot;, &quot;en&quot;, &quot;legal/models&quot;)\
    .setInputCols([&quot;document&quot;, &quot;token&quot;])\
    .setOutputCol(&quot;class&quot;)\
    .setCaseSensitive(True)\
    .setMaxSentenceLength(512)

clf_pipeline = nlp.Pipeline(stages=[
    document_assembler, 
    tokenizer,
    sequence_classifier    
])

empty_df = spark.createDataFrame([['']]).toDF(&quot;text&quot;)

model = clf_pipeline.fit(empty_df)

text_list = [
&quot;&quot;&quot;This Agreement will be binding upon and inure to the benefit of each Party and its respective heirs, successors and assigns&quot;&quot;&quot;,
&quot;&quot;&quot;Activity that is in direct competition with the Company's business, including but not limited to developing, marketing, or selling products or services that are similar to those of the Company.&quot;&quot;&quot;
]

df = spark.createDataFrame(pd.DataFrame({&quot;text&quot; : text_list}))

result = model.transform(df)
```

&lt;/div&gt;

## Results

```bash
+--------------------------------------------------------------------------------+-----------------+
|                                                                            text|            class|
+--------------------------------------------------------------------------------+-----------------+
|This Agreement will be binding upon and inure to the benefit of each Party an...|            OTHER|
|Activity that is in direct competition with the Company's business, including...|NON_COMPETE_ITEMS|
+--------------------------------------------------------------------------------+-----------------+
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|legclf_nda_non_compete_items_bert|
|Compatibility:|Legal NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[document, token]|
|Output Labels:|[class]|
|Language:|en|
|Size:|406.4 MB|
|Case sensitive:|true|
|Max sentence length:|512|

## References

In-house annotations on the Non-disclosure Agreements

## Benchmarking

```bash
label              precision  recall  f1-score  support 
NON_COMPETE_ITEMS  1.00       1.00    1.00      10      
OTHER              1.00       1.00    1.00      64      
accuracy           -          -       1.00      74      
macro avg          1.00       1.00    1.00      74      
weighted avg       1.00       1.00    1.00      74  
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="legal" /><category term="licensed" /><category term="bert" /><category term="classification" /><category term="nda" /><category term="non_compete" /><category term="tensorflow" /><summary type="html">Description Given a clause classified as NON_COMP using the legmulticlf_mnda_sections_paragraph_other classifier, you can subclassify the sentences as NON_COMPETE_ITEMS, or OTHER from it using the legclf_nda_non_compete_items_bert model. It has been trained with the SOTA approach. Predicted Entities NON_COMPETE_ITEMS, OTHER Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) tokenizer = nlp.Tokenizer()\ .setInputCols([&quot;document&quot;])\ .setOutputCol(&quot;token&quot;) sequence_classifier = legal.BertForSequenceClassification.pretrained(&quot;legclf_nda_non_compete_items_bert&quot;, &quot;en&quot;, &quot;legal/models&quot;)\ .setInputCols([&quot;document&quot;, &quot;token&quot;])\ .setOutputCol(&quot;class&quot;)\ .setCaseSensitive(True)\ .setMaxSentenceLength(512) clf_pipeline = nlp.Pipeline(stages=[ document_assembler, tokenizer, sequence_classifier ]) empty_df = spark.createDataFrame([['']]).toDF(&quot;text&quot;) model = clf_pipeline.fit(empty_df) text_list = [ &quot;&quot;&quot;This Agreement will be binding upon and inure to the benefit of each Party and its respective heirs, successors and assigns&quot;&quot;&quot;, &quot;&quot;&quot;Activity that is in direct competition with the Company's business, including but not limited to developing, marketing, or selling products or services that are similar to those of the Company.&quot;&quot;&quot; ] df = spark.createDataFrame(pd.DataFrame({&quot;text&quot; : text_list})) result = model.transform(df) Results +--------------------------------------------------------------------------------+-----------------+ | text| class| +--------------------------------------------------------------------------------+-----------------+ |This Agreement will be binding upon and inure to the benefit of each Party an...| OTHER| |Activity that is in direct competition with the Company's business, including...|NON_COMPETE_ITEMS| +--------------------------------------------------------------------------------+-----------------+ Model Information Model Name: legclf_nda_non_compete_items_bert Compatibility: Legal NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [document, token] Output Labels: [class] Language: en Size: 406.4 MB Case sensitive: true Max sentence length: 512 References In-house annotations on the Non-disclosure Agreements Benchmarking label precision recall f1-score support NON_COMPETE_ITEMS 1.00 1.00 1.00 10 OTHER 1.00 1.00 1.00 64 accuracy - - 1.00 74 macro avg 1.00 1.00 1.00 74 weighted avg 1.00 1.00 1.00 74</summary></entry></feed>